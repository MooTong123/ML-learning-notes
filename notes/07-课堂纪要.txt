4.2 决策树分类原理【*****】
    1.信息增益
        信息增益 = entroy(前) - entroy(后)
        注意：信息增益越大，我们优先选择这个属性进行计算
        信息增益优先选择属性总类别比较多的进行划分
    2.信息增益率
        维持了一个分离信息度量，通过这个分离信息度量当分母，进行限制
    3.基尼增益
        1.基尼值：
            从数据集D中随机抽取两个样本，其类别标记不一致的概率
            Gini（D）值越小，数据集D的纯度越高。
        2.基尼指数：
            选择使划分后基尼系数最小的属性作为最优化分属性
        3.基尼增益：
            选择基尼增益最大的点，进行优化划分
        4.基尼增益构造过程：
            1.开始将所有记录看作一个节点
            2.遍历每个变量的每一种分割方式，找到最好的分割点
            3.分割成两个节点N1和N2
            4.对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止。
        5.决策树的变量可以有两种，分别对应的划分方式：
            1.数字型
                通过对数据取两个数字之间的中间值，进行划分
            2.名称型
                通过对属性的类别进行划分
        6.如何评估分割点的好坏？
            主要看分割的是否纯
    4.三种算法对比：【****】
        ID3 算法
            采用信息增益作为评价标准
            只能对描述属性为离散型属性的数据集构造决策树
            缺点是倾向于选择取值较多的属性
        C4.5算法
            用信息增益率来选择属性
            可以处理连续数值型属性
            采用了一种后剪枝方法
            对于缺失值的处理
            缺点是：C4.5只适合于能够驻留于内存的数据集
        CART算法
            C4.5不一定是二叉树，但CART一定是二叉树
            是信息增益的简化版本
4.3 cart剪枝
    1.剪枝原因
        噪声、样本冲突，即错误的样本数据
        特征即属性不能完全作为分类标准
        巧合的规律性，数据量不够大。
    2.常用剪枝方法
        预剪枝
            在构建树的过程中，同时剪枝
                eg:
                限制节点最小样本数
                指定数据高度
                指定熵值的最小值
        后剪枝
            把一棵树，构建完成之后，再进行从下往上的剪枝

4.4 特征工程-特征提取【***】
    1.特征提取
        将任意数据（如文本或图像）转换为可用于机器学习的数字特征
    2.特征提取分类:
        字典特征提取(特征离散化)
        文本特征提取
        图像特征提取（深度学习将介绍）
    3.api
        sklearn.feature_extraction
    4.字典特征提取
        字典特征提取就是对类别型数据进行转换
        api:
            sklearn.feature_extraction.DictVectorizer(sparse=True,…)
            aparse矩阵
                1.节省内容
                2.提高读取效率
            属性：
                DictVectorizer.get_feature_names() 返回类别名称
        注意：
            对于特征当中存在类别信息的我们都会做one-hot编码处理
    5.文本特征提取（英文）
        api:
            sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
                stop_words -- 停用词
                注意：没有sparse这个参数
                    单个字母，标点符号不做统计
    6.文本特征提取（中文）
        注意：
            1.在中文文本特征提取之前，需要对句子（文章）进行分词（jieba）
            2.里面依旧可以使用停用词，进行词语的限制
    7.tfidf
        1.主要思想：
            如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
        2.tfidf
            tf -- 词频
            idf -- 逆向文档频率
        3.api
            sklearn.feature_extraction.text.TfidfVectorizer
        注意：
            分类机器学习算法进行文章分类中前期数据处理方式
4.5 决策树算法api【*】
    sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)
        参数：
        criterion
            特征选择标准
        min_samples_split
            内部节点再划分所需最小样本数
        min_samples_leaf
            叶子节点最少样本数
        max_depth
            决策树最大深度
4.6 案例：泰坦尼克号乘客生存预测【***】
    1.流程分析
        1.获取数据
        2.数据基本处理
        2.1 确定特征值,目标值
        2.2 缺失值处理
        2.3 数据集划分
        3.特征工程(字典特征抽取)
        4.机器学习(决策树)
        5.模型评估
    2.可视化
        sklearn.tree.export_graphviz()
    3.小结
        优点：
            简单的理解和解释，树木可视化。
        缺点：
            决策树学习者可以创建不能很好地推广数据的过于复杂的树,容易发生过拟合。
        改进：
            减枝cart算法
            随机森林（集成学习的一种）

5. 集成学习
5.1 集成学习算法简介
    1.什么是集成学习
        超级个体和弱者联盟对比，后者更优
    2.复习：机器学习两个核心任务
        1.解决欠拟合问题
            弱弱组合变强
            boosting
        2.解决过拟合问题
            互相遏制变壮
            Bagging
5.2 Bagging【**】
    1.bagging集成过程
        1.采样
            从所有样本里面，采样一部分
        2.学习
            训练弱学习器
        3.集成
            使用平权投票
    2.随机森林介绍
        1.随机森林定义
            随机森林 = Bagging + 决策树
        2.流程：
            1.随机选取m条数据
            2.随机选取k个特征
            3.训练决策树
            4.重复1-3
            5.对上面的若决策树进行平权投票
        3.注意：
            1.随机选取样本，且是有放回的抽取
            2.选取特征的时候吗，选择m<<M
                M是所有的特征数
        4.api
            sklearn.ensemble.RandomForestClassifier()
    3.bagging的优点
        Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法
        1.均可在原有算法上提高约2%左右的泛化正确率
        2.简单, 方便, 通用

